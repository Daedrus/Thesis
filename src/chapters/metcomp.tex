\chapter{Methods}
\label{chapter:methods}

In this chapter we will focus on the description of methods used in obtaining dynamic memory allocation information. We will start by presenting approaches used by profilers in general and see how these are applicable to memory profiling in particular. Given the fact that we are concerned in both the size and the locations of memory allocations and since these two involve different approaches, we will present different methods of obtaining relevant information separately for these two. Finally, a test program and the platform on which it was run will be described, upon which all these methods will be tested. This will provide a good performance reference in order to determine the overhead the memory profiling techniques induce.

\section{Profiling methods}
\label{section:profilingmethods}

Profiling a computer program means analyzing its behaviour during runtime in order to obtain information that can lead to its optimization. The sought information can vary from memory usage to identifying the most commonly taken call paths, all the way down to cache misses and the use of particular processor instructions. Any interaction between the program and the OS and, consequently, the hardware, can potentially be analyzed and, based on the results, improved if possible.

We can make a first classification of profilers based upon their intrusiveness. Thus, we have intrusive profilers which modify in some way the program's instructions in order to insert code which helps with the analysis. These profilers can be classified further into those that need a program's source code to perform the analysis and those that can work on a program's binary form. Finally, we have non-intrusive profilers which require no modification of the original program.

Another classification can be made based on the technique profilers use to collect information. We can thus have:
\begin{itemize}
\item \textit{code instrumentation} profilers which add or modify instructions in the original program to collect the required information
\item \textit{statistical} profilers which work by periodically sampling the program and then extrapolate conclusions statistically
\item \textit{performance counters} are special registers in modern processors which keep track of specific events such as cache misses. These can be used to observe behavioral patterns in programs.
\item \textit{hardware assisted} profilers use dedicated hardware to collect and analyze information about running programs
\item \textit{event based} profilers use predetermined hooks provided by the underlying software and hardware platforms to collect data on the running program
\end{itemize}

This classification is not precise as many profilers use, for example, both performance counters and code instrumentation to create more detailed profiles. In the following sections we will describe these techniques in more detail and see how and if they relate to our goal of low overhead memory allocation profiling.

A tool for deducing memory usage behaviour based on analysis of code behaviour.
zhao-cgo07-umi.pdf [Ubiquitous memory introspection]

Trace analysis discussion
wenke.pdf [Framework for instruction-level tracing and analysis of program executions]

\subsection{Code instrumentation}
\label{subsection:codeinstrumentation}

Definition of instrumentation in general.
Code instrumentation is the process of altering a program (by adding code or modifying existing code) in order to collect performance statistics. There are several ways in which this can be done:
\begin{itemize}
\item \textit{source level} instrumentation involves modifying a program's actual source code before or during compilation. Having direct access to a program's source code has the big advantage of being able to monitor application-specific statistics.
\begin{itemize}
\item \textit{manual} instrumentation can be done by the programmer by simply adding instructions which monitor different statistics at points of his choosing. The advantage of this approach is that it can be very fast since it requires no external tool to be run alongside the program and the inserted code can be taylored to the application's specifics so it can take advantage of things such as garbage collection runs. The disadvantage is that it requires deep knowledge of an application's source code in order to identify the points where instrumenting code should be added. Thus, it is heavily intrusive but has the advantage of potentially having very low overhead.
\item \textit{tool assisted source level} instrumentation involves using external tools to insert instrumentation code in the program's source code. For example, through a specific language the tool could be guided to monitor the number of times a specific function has been called during a run of the program. Thus, there is a shift from the programmer actually modifying the program's source code to instructing a tool in what way it should modify the program so that the desired statistics are collected. The advantage of this approach over manual instrumentation is that no detailed knowledge of the source code is required but with this flexibility is lost. For example, monitoring complicated internal data structures might involve complex tool scripts or not even be possible at all. Another approach is to have a tool that analyzes the source code in order to indicate the best points for instrumentation code to be added. This approach has been described by Larus and Ball\cite{Ball94}.
\item \textit{compiler assisted} instrumentation can be viewed as another form of tool-assisted instrumentation where the tool is the compiler itself. GCC has the option of adding code to a program which then allows the program to output profiling information which can be analyzed offline by a call graph profiler called gprof\cite{Graham04}. The information provided by this tool is related to the times spent in each of the program's functions and the way the functions interact with each other.
\end{itemize}
\item \textit{binary level} instrumentation involves modifying a program's source code in binary form offline or during runtime. The main advantage of this approach is that it does not require access to the source code so programs that do not have their source publicly available can be analyzed too. The downside is that the complexity of these tools is large since the binaries have to be carefully analyzed before they are modified. Because of this and the fact that it is very difficult to draw conclusions about an application from its executable, application-specific monitoring (such as the data structure example above, or, another example, determining how much memory a tab consumes inside a web browser) is very difficult using this approach.
\begin{itemize}
\item \textit{binary alteration} means modifying a program's binary before it is run. ATOM\cite{Srivastava94} is a tool which allows instrumentation code to be added to applications using only link-time information. To be more exact it is a tool for building profiling tools. It works by providing a framework for the definition of instrumentation routines and for merging these routines with the program to create an instrumented executable. The LOPI framework\cite{Kagstrom05} implements a similar solution.
\item \textit{binary code injection} tools add code to a program while it is running. Dyninst\cite{Buck00}, for example, uses a concept known as code trampolines to perform this task. The idea is that simply replacing code in the original binary with a jump instruction to the routine that performs the profiling is not possible because of the possibility of overwriting in-use registers. Thus, the jump instruction points instead to a piece of code known as a trampoline. This piece of code has the responsability of saving the context from the jump point and restoring it after the profiling function has been executed. Several implementation schemes of this are possible such as the use of a separate mini-trampoline whose task is to execute the original replaced instruction. No matter the implementation, however, the idea of preserving the context and thus the correctness of the program remains. A similar technique called springboards has been attempted for use in kernel profiling\cite{Tamches99}. A unified solution for both userspace and kernelspace code profiling has been implemented by the DTrace tool\cite{Cantrill04}.
\item \textit{runtime translation} is a method which involves converting a program's instructions into another representation which is more suitable for profiling. Valgrind\cite{Nethercote07} and PIN\cite{Luk05} are tools which implement this technique. Valgrind, for example, uses a method through which every register and memory value is shadowed and thus can be monitored. This allows for very powerful memory leak detectors to be implemented. The downside to the code conversion is that it incurs a significant time penalty, thus, even a tool that does nothing in Valgrind (called nullgrind) slows down the program significantly enough that it cannot be used for live analysis \cite{Newsome05}.
\end{itemize}
\end{itemize}

Present paper as trying to optimize instrumentation so that it has lower overhead.
p28-kumar.pdf [Low overhead program monitoring and Profiling]

Present the article as directly treating the issue of memory allocation profiling.
mprof.pdf [Memory allocation profiler for C and LISP]

This presents some improvements on the translation part.
VEE2006.pdf [HDTrans, low-leve dynamic instrumentation system]

\subsection{Statistical profiling}
\label{subsection:statisticalprofiling}

Another technique used in profiling is to periodically sample the statistics we are interested in. For example, every X miliseconds, the program can be stopped and its stack trace can be inspected. By doing this  multiple times, we can deduce how much time the program spends in every routine. Of course, the precision of this approach is given by the frequency of the sampling. The downside is that the overhead increases with frequency.

There exist different approaches to how the sampling can be done:
\begin{itemize}
\item \textit{period sampling} is the simplest method, where the period is chosen randomly and then modified empirically until a good balance between overhead and results has been reached. This approach has been used by Whaley\cite{} to implement a profiler for Java virtual machines which focuses on an efficienty way of organizing and storing stack trace information.
\item \textit{bursty tracing} uses two variables: one which specifies the sampling rate and another to specify how long the sampling should last. It is used in conjunction with instrumented code which is only executed when sampling is enabled. Adaptive versions of this have multiple sampling rates and durations for different code areas in order to selectively control the analysis frequency of those areas which are considered to be more important. Chilimbi and Hauswirth\cite{Hauswirth04} have used this approach in order to implement memory leak detection by using a tree-based heap model which stores information about access frequency of objects on the heap. This access frequency is updated through sampling-enabled instrumentation code. Objects which have not been accessed in a long time (either because they truly have not been accessed or the sampling missed their accesses) are reported. The idea of using two versions of the code and switching between them based on a sampling rate was originally presented by Arnold and Ryder\cite{Arnold01}.
\item \textit{stride based sampling} uses three parameters: one for the sampling rate, one to specify a count-down mechanism for sampling every n-th method call (the stride) and another one to give the length of the profiling window. The sampling rate is usually determined by a timer, eliminating the need of maintaining a counter. This approach has been used by Arnold and Grove\cite{Arnold05} to implement call graph profiling in virtual machines.
\end{itemize}

Tools which rely on statistical profiling for some of the information they provide are AMD CodeAnalyst, Intel VTune and gprof, which we presented eariler in code instrumentation. Gprof actually uses sampling to determine the time spent in certain functions while it uses counting based instrumentation methods to keep track of how often a certain function has been called.

Using sampling in order to determine the exact memory consumption of a program would mean sampling a number of allocations for a certain period of time and then deducing the total number of bytes used from those allocations. There is the possibility of the profiling window completely missing important allocations so precise calculations are not possible. It would be possible to determine the average number of allocated bytes per unit of time and then draw conclusions from that but again, we are interested in byte-level precision. However, as later subchapters show, there is a role that sampling based profiling plays in our goal and that is to determine how often we should trigger computations which aid in determining a program's running size.

\subsection{Performance counters}
\label{subsection:performancecounters}

What they are, how they are used. Examples from modern x86 architectures maybe. Oprofile et. al.

Performance counters are special registers found on modern platforms which keep track of CPU cycles, completed instructions, instruction cache misses, data cache misses, TLB misses and many more. They either count events or cycles so, for example, cache counters usually come in both forms, one for keeping the number of cache misses and another for keeping the total number of cycles lost due to these misses. Some architectures provide counters which are configurable. These counters are not tied to monitoring a specific event but they can be configured to monitoring any event from a pre-determined list.

Itzkowitz and Wylie\cite{Itzkowitz03} describe the difficulties of using performance counters, including a solution for handling their overflow. They provide an implementation of a data collector and analyzer which ties performance counters' values with discrete instructions from a given program.

London and Moore have proposed a unified framework for cross-platform hardware performance counters accessibility\cite{London01}. This framework aims to abstract away the low-level details of accessing the counters and to provide their values in a uniformy way across different platforms.

While these counters by themselves provide very little information directly related to memory allocations they can be used as data which drives other tools to implement memory optimizations. For example, Tikir and Hollingsworth\cite{Tikir04} used such counters to profile the memory access behaviour of an application and then based on this profile, move the most frequently accessed memory pages into caches closer to the processor.

Mention oprofile.

\subsection{Hardware-assisted profiling}
\label{subsection:hardwareassistedprofiling}

One step forward from performance counters is to have complex dedicated hardware components which aid the profiling process. Rather than being just simple counters, these hardware components can range from simple auxiliary microprocessors to completely using existing processors from multi-core architectures for profiling purposes.

Combination of hardware and software for program monitoring.
science.pdf [Efficient dynamic program monitoring on multi-core systems]

Yet another software hardware combination.
10.1.1.104.4146.pdf [Dise: A Programmable Macro Engine for Customizing Applications]

HW SW approach for detecting memory leaks.
shetty\_ibmpac04.pdf [HeapMon: memory bug detector]

venkataramani\_hpca07.pdf [hardware for memory access monitoring and debugging]

LBA\_ISCA08.pdf [Flexible Hardware Acceleration for Instruction-Grain Program Monitoring]

LBA\_asid2006.pdf [Log-Based architectures for general-purpose monitoring of deployed code]

tacodec04.pdf [Architectural support for dynamic monitoring]

profiler.hpca.pdf [A programmable co-processor for profiling]

\subsection{Event-based profiling}
\label{subsection:eventbasedprofiling}

Events are triggered either through software or hardware exceptions. Give examples of how these might be used.

Describes a device driver which interecepts interrupts to collect statistics.
Continuous profiling: where have all the cycles gone

\section{Heap profiling}
\label{section:heapprofiling}

In section \ref{section:memlayout} we have presented the typical layout of a program after it has been loaded into memory. While in modern programs there exist a lot more sections than the ones described, the heap is usually the one where most of the allocations are done. Thus, we will not concern ourselves with the other sections because their size is pre-determined from compile time and they do not suffer modifications during run-time. In this subchapter we will present different methods of determining the size and the point in the program where allocations on the heap are performed.

\subsection{Allocation size profiling}
\label{subsubsection:allocationsizeprofiling}

The first problem we want to solve is the problem of determining the total size of all the data that exists on the heap. More specifically, we want to be able to answer one of our original questions: how much memory have the classes/modules allocated on the heap?

\subsubsection{Overloading memory allocation routines}
\label{subsubsection:overridingroutines}

A first solution to keeping track of all the allocations that a program has done is to overload the routines that do the allocations. By doing this, we can insert our own code in the routines, code which allows us to manipulate the allocation information in any way we want. The routines which have to be overloaded are the same ones presented in section \ref{section:heap}.

There are several problems with this approach, one of them related to the actual implementation of the mechanism. Overloading the routines means replacing them with our own while keeping the functionality intact. This has to be done in a way that is transparent to the running program and has very little overhead, preferably none. Different approaches exist:
\begin{itemize}
\item The \textit{new} and \textit{delete} operators can be overridden globally through language constructs provided by C++ itself. By looking at the way these two operators are implemented in the standard C++ library, one could provide an implementation that is identical but also provides additional profiling code.
\item For the \textit{malloc}, \textit{realloc} and \textit{free} routines, GCC provides hooks which allows their behaviour to be modified. These hooks are actually variables declared in malloc.h : \_\_malloc\_hook, \_\_realloc\_hook, \_\_free\_hook, \_\_memalign\_hook. All of these can point to independent routines which are called whenever the original allocation routines are called. These routines' signature contains a caller parameter which is the return address found on the stack when the allocation routines were called, thus allowing allocation point tracking\cite{GCCman}. The downside with using this method is that it is GCC specific, so if other compilers are used then either a similar mechanism has to exist for them or this approach does not work.
\item A separate library providing implementations for all the C-level allocation routines can be used. Since \textit{new} and \textit{delete} are also using these, they will also be taken into account, thus covering the whole range. This library can then be linked with the original program in such a way that the overloaded routines are used instead of the ones provided by the standard library. This is the approach that Valgrind uses, by exporting symbols which take precedence over the ones in glibc.so\cite{Seward02}. While it does have the benefit of being unintrusive it still is dependant on the build system, especially on the linker used.
\item Another solution is to provide wrappers for the allocation routines, which will be used instead of the original ones. The downside to this is that it is very intrusive since all of the original calls have to be replaced with calls to the wrappers. Tools that do this replacement automatically can be used.
\end{itemize}

Hunt and Brubacher\cite{Hunt99} classify techniques of intercepting function calls on Windows into four categories:
\begin{itemize}
\item \textit{Call replacement in application source code} - All of the above, except for the one involving providing a separate library fit into this category.
\item \textit{Call replacement in application binary code} - By using symbolic information call sites are identified and jump code to profiling routines can be inserted
\item \textit{DLL redirection} - Similar to using a separate library, the internals of this technique are Windows-specific
\item \textit{Breakpoint trapping} - By inserting a debugging breakpoint in the function we wish to intercept, we can have the debug exception handler reroute to a profiling routine. This involves a separate process (the Windows debugger) and it has the downside of suspending all application threads.
\end{itemize}
More than that, they compare these techniques with their interception implementation and show that the overhead varies from 250ns to 400ns with call replacement and DLL redirection, while breakpoint trapping has an overhead on the order of microseconds. If we add to this the fact that the profiling routine itself induces overhead, along with the fact that it proves to be not-trivial to implement and sometimes even intrusive, we can conclude that overloading the memory allocation routines in order to obtain live heap information is not a viable solution.

\subsubsection{On-demand memory tracking}
\label{subsubsection:ondemandtracking}

We now take a different approach to keeping track of the amount of allocated memory, one which does not involve interfering with the allocation routines. To do that, we note that most of the data living on the heap is structured in some way. Whether it is stored in just a simple array of integers or more complex data structures, it has references to it which can be accessed to determine its size. The advantage of such an approach is that we control when the size is determined and thus implicitly control when the overhead of this computation is imposed. The idea is to trigger the computation of the data structure's size on-demand, shifting the constant overhead of overloading memory allocation routines to a one-shot significantly larger overhead which could potentially be triggered during a period of low processor utilization.

The first possible way of keeping track of a data structure's size is \textit{counter-based}. This is as simple as keeping a variable which keeps track of the size that the data structure occupies, counter which is updated accordingly for each modification of said data structure. For example, an addNode function for a linked list would increment the variable with the size of the newly added node, while a removeNode function would decrement it in a similar manner. Naturally, more complex structures would require perhaps more counters and an even more careful accounting method, but the idea is the same: have a set of variables which accurately represent the size of the data structure at any point in time. The biggest advantage of this method is that it has very low overhead. The bulk of the accounting is spread between the methods which update the data structure and usually this involves only incrementing or decrementing the variables. When the information related to the data structure's size is required on-demand, all there is to do is to return the variables which contain this information, making this approach very lightweight in terms of overhead. The downsides are that it is intrusive, but, more importantly, it is very hard to maintain. Experience has shown \cite{Nethercote12} that people forget to update the profiling code when the data structure is updated, or partially update the profiling code since it is spread out in many methods that have an impact on the data structure. This leads to incorrect reporters that might not even be acknowledged as incorrect until after some serious debugging.

Since the main problem with the above method was that the profiling was spread in so many places that it was hard to keep track of all of them when they needed to be updated, perhaps there is a way to aggregate all of the profiling into one place. This is the idea with \textit{traversal-based} profiling. Have one method (or several, if multiple statistics are monitored) which traverses the data structure and reports its size. This does have significantly larger overhead than the above technique, especially if the data structure is large, but it is easier to maintain. Also, let's not forget that the idea is to trigger this traversal on-demand. There are several complicating factors with this approach, such as:
\begin{itemize}
\item Cycles in the data structure could lead to the same memory being counted twice
\item When using inheritance, the sub-classes must make sure not to take into account the memory of their parents again
\item Complex structures require complex traversals which are not trivial to implement and therefore might be difficult to maintain too
\end{itemize}

Note that by using these methods we have now lost the ability to detect memory leaks. If we would have kept track of every allocation then this extension would have been possible with some effort. However, this was never the purpose of this thesis so memory leak detection is out of scope. Allocations which are done and then never freed and do not have a reference to them will still continue to live on the heap and will occupy space but will not be detected by the profiler. This is considered a programmer error and specialized tools for their detection do exist.

In conclusion, both of the above methods are highly intrusive, requiring access to the source code. Counter-based profiling is the lightest of the two, but the hardest to maintain, while traversal-based has a higher overhead but better maintainability. Which one should be chosen is a matter of the project's size and priorities.

\subsection{Allocation point profiling}
\label{subsection:allocationpointprofiling}

The second problem we want to solve is to be able to answer two of our initial questions: who did the allocation and what lead to the allocation being done? The answer to both of these questions is found in the stack trace from the moment the allocation is done.

\subsubsection{Manual stack traversal}
\label{subsubsection:manualstacktraversal}

As we have seen in section \ref{section:stack}, the stack is where we can find information about the call chain that led to an allocation. Accessing the stack is, unfortunately, not a straight forward endeavour, mostly because each platform has subtle differences in the way the stack is implemented, which makes accessing it a bit difficult. Some compilers provide already implemented routines which hide away the details of the underlying architecture. One such example is GCC, which provides the \textit{backtrace} function. This function returns the call chain in a buffer of a given size. What it actually does behind the scenes is perform a stack walk.

A piece of software which does not want to be tied to a specific compiler should not use such compiler-provided functions but instead opt to implement its own. To give an idea of the complexity of a stack walker we now present the C implementation of such a program on an x86 Linux platform.

Keeping in mind the structure of a stack frame, described in section \ref{section:stack}, we need to determine two things:
\begin{itemize}
\item how to jump from stack frame to stack frame
\item how to obtain the return address from each frame, knowing that this return address is what determines the caller
\end{itemize}

Knowing only the beginning of the stack is of no use to us since we do not know how much local data has been pushed on the stack and therefore cannot determine precisely where the return address is. In this case, we can use the ebp register which is commonly used to point at the beginning of the current local data. However, we know that just above the local data lie the ebp value of the caller and the return address. Thus, to jump from stack frame to stack frame we have to follow the ebp values and to get the return address we just look at the value above the ebp on the stack.

Without going into the exact implementation details, a solution that does this is shown in \ref{stackwalker}:
\begin{lstlisting}[label=stackwalker,caption=Simple stack walker for x86]

struct frame {
        struct frame* old_fp;
        long ip;
};

struct frame *frame, *fp;

asm("movl %%ebp, %0" : "=r"(frame));

fp = frame;

for(; !(fp < frame) && !(fp < stack_bottom));
      fp = (struct frame *)((long)fp->old_fp))
   {
       // Do something with the return address from fp->ip
   }
\end{lstlisting}

The end result is that we can obtain a list of return addresses which can then be further used to obtain the actual names of the routines forming the call chain. Inserting the above routine in every allocation point would give us a stack trace which can be used to determine the exact call chain leading to the allocation.

There are, however, several downsides to this approach. First of all it is heavily platform dependant. The above code only runs on x86, using specific GCC directives. Not only that, but it relies on the fact that the code has been compiled with frame pointers activated. Some compiler-level optimizations remove the frame pointers to reduce stack frame size and obtain a small increase in speed. Different hardware platform may have a completely different stack frame format so the code would have to be rewritten for each compiler/platform combination, leading to something that would probably be very hard to maintain. The second problem is overhead. Attaching this code to every allocation point can lead to unnecessary overhead especially if we are not interested in the associated stack traces. A better method would be to activate the stack tracing on-demand, just for those allocations in which we are interested.

\subsubsection{Low overhead tracepoints}
\label{subsubsection:lowoverheadtracepoints}

The problem of low overhead tracepoints has been under discussion for a long time, especially in the context of debugging. The DTrace tool for Solaris allows probes to be inserted into a running program which have low overhead when they are disabled\cite{Cantrill04}. Such implementations have also been attempted on SPARC\cite{Kessler90} and the LTTng project had a series of tools dedicated to tracing Linux both in userspace and kernel\cite{Desnoyers09}. We will present the approach currently taken by the Linux kernel in this section.

A naive implementation of an on-demand triggerable tracepoint would just check the truth value of a flag and, based on that value, either call the tracing routine or not. It could be something as simple as the code in \ref{tracepoint0}. There are some problems which stem from this such as the need of a data structure which keeps a list of all the available tracepoints and implements some naming scheme allowing the user to enable/disable them independently. The question is if there is some way to avoid the condition check so that a disabled tracepoint would have even lower overhead.
\begin{lstlisting}[label=tracepoint0,caption=Naive tracepoint implementation]
...
if (tracepoint_enabled)
	trace();
...
\end{lstlisting}

The idea is to keep a list of all the statically defined tracepoints. In our case, since we want all allocation points to be traceable, there will be a tracepoint for each of them. This list is built by the compiler during compilation and placed in a special section in the executable which can be accessed during runtime. At the same time, tracepoints which are disabled are replaced with nop operations. To activate a tracepoint during runtime one has to lookup its address in the tracepoint table and replace the nop instruction from that address with a jump to the place in the code which calls the tracing function. The key to having this work is special compiler support for moving code which can be jumped to, but not accessed directly, out of line\cite{GCCasmgoto}. The listing from \ref{tracepoint1} shows the way this is done in the Linux kernel, along with a typical usage scenario.
\begin{lstlisting}[label=tracepoint1,caption=Linux kernel jump label implementation]
static __always_inline bool static_branch(struct jump_label_key *key)
{
        asm goto("0: nop\n\t"
                 ".pushsection __jump_table, \"aw\" \n\t"
                 ".balign 4 \n\t"
                 ".long 0b, %l[l_yes], %c0 \n\t"
                 ".popsection \n\t"
                 : : "i" (key) : : l_yes);
        return false;
l_yes:
        return true;
}

#define TRACE(name)
        static const char
        __tpname_##name[]
        __attribute__((section("__tracepoints_strings"))) =
                #name;

        static struct tracepoint
        __tracepoint_##name
        __attribute__((section("__tracepoints"))) =
                { 0, __tpname_##name, { 0 } };

        static struct tracepoint * const
        __tracepoint_ptr_##name
        __attribute__((section("__tracepoints_ptrs"))) =
                &__tracepoint_##name;

        if (static_branch(&__tracepoint_##name.key))
                trace(&__tracepoint_##name);
\end{lstlisting}

It works by inserting a nop at the label defined by \textit{0:}. In the section called \textit{\_\_jump\_table} we save the address of that label, the address of the label to jump to when the tracepoint is activated, along with a key identifying the tracepoint uniquely. Since the routine is typically used in a branch, and since it will always evaluate to false, a compiler will want to remove the code completely since it is unreachable. However, due to the jump to the \textit{l\_yes} label, it is not removed completely but moved somewhere out of line thus leaving only the nop instruction in place. We know where the code is moved because we have saved the address of the \textit{l\_yes} label, thus, in order to activate the tracepoint we have to replace the nop with a jump to that address.

This implementation shows that it is possible to achieve a tracepoint implementation whose only significant overhead is related to the code size of the nops and the out of line code. The downside is the same as the stack walker's: the implementation is platform specific. In this case it might even be worse since the optimization that the GCC compiler does by moving the code out of line and allowing labels inside an assembly block might not even be possible in other compilers. Permission to modify the program's code during runtime is also required and this might not be allowed in secure environments. The main issue is thus one of maintainability and of deciding if the cost of implementing and maintaining such a solution is indeed lower than the benefit of being able to trace call chains in key points of the program.

One final note to keep in mind is that the question we are asking is if it is possible to implement the above and tie it into a piece of software so that it can be used live and without damaging its performance. There are tools which already do this sort of tracing, such as DTrace mentioned above and even Valgrind so it is not an issue of implementation but rather performance and maintainability.

\subsubsection{Global stack object}
\label{subsubsection:globalstackobject}

The above two solutions suffer most on the maintainability side because of their platform dependencies. The question which naturally follows is if we can abstract away those parts into something which is independent of the platform we are running on. The answer to this would be to keep our own pseudo-stack (or stacks in case of multiple threads) which is globally accessible and can be queried regarding its state at any time. We say pseudo-stack because we would only be keeping the function names in it since that is what we are interested in. To have this working, each function must call one routine at its entry point pushing its name on the stack and another at its exit point for popping. A tool which inserts these calls automatically can be created.

The global stack object thus removes the need of having a stack walker. However, invoking the object for providing the call chain still requires the tracepoints and making these platform independent leads us to the naive implementation from listing \ref{tracepoint0}.

\section{Test program description}
\label{section:testprogram}
