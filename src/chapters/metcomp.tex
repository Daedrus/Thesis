\chapter{Methods}
\label{chapter:methods}

In this chapter we will focus on the description of methods used in obtaining dynamic memory allocation information. We will start by presenting approaches used by profilers in general and see how these are applicable to memory profiling in particular. Given the fact that we are concerned in both the size and the locations of memory allocations and since these two involve different approaches, we will present different methods of obtaining relevant information separately for these two. Finally, a test program and the platform on which it was run will be described, upon which all these methods will be tested. This will provide a good performance reference in order to determine the overhead the memory profiling techniques induce.

\section{Profiling methods}
\label{section:profilingmethods}

Profiling definition.

In this section we will describe the most common techniques used for profiling today. Some of these techniques do not target memory profiling specifically, but nevertheless they can be used for this purpose.

Types of profiling + short description for each.

mprof.pdf [Memory allocation profiler for C and LISP]

p28-kumar.pdf [Low overhead program monitoring and Profiling]

science.pdf [Efficient dynamic program monitoring on multi-core systems]

zhao-cgo07-umi.pdf [Ubiquitous memory introspection]

10.1.1.104.4146.pdf [Dise: A Programmable Macro Engine for Customizing Applications]

shetty\_ibmpac04.pdf [HeapMon: memory bug detector]

WRL-94-2.pdf [ATOM, system for implementing customized program analysis tools]

paper.pdf [gprof - a call graph execution profiler]

wenke.pdf [Framework for instruction-level tracing and analysis of program executions]

dynamopldi.pdf [Dynamo optimization system]

10.1.1.90.5250 [Optimally profiling and tracing programs]

Continuous profiling: where have all the cycles gone (paywalled, have to check liu online library)
System support for automatic profiling and optimizatins: paywalled

\subsection{Code instrumentation}
\label{subsection:codeinstrumentation}

Definition of instrumentation in general.

Types of instrumentation + description for each of them.

Give examples of tools and how they work.

valgrind2007.pdf [How Valgrind works]

shadow-memory.pdf [How to shadow every byte of memory]

a5-venkataramani.pdf [MemTracker, tool based on PIN]

interact-9\_IEEE\_23210082 [Automatic Low overhead instrumentation using the LOPI Framework]

Tamches99FineGrained.pdf [Fine-grained dynamic instrumentation of kernels]

apiPreprint.pdf [An API for Runtime Code Patching]

phd2004.pdf [Nethercote dynamic instrumentation phd]

derek-phd-thesis [Efficient, transparent, comprehensive runtime code manipulation]

dtrace\_usenix [Dynamic instrumentation of production systems]

10.1.1.85.4883 [PIN, building customized program analysis tools with dynamic instrumentation]

VEE2006.pdf [HDTrans, low-leve dynamic instrumentation system]

Reversible debugging using program instrumentation - paywalled

\subsection{Statistical profiling}
\label{subsection:statisticalprofiling}

Definition of statistical profiling.

How it works.

Examples of tools.

swat\_asplos\_final.pdf [Leak detection using statistical profiling]

dcs-tr-424.pdf [Reducing the cost of instrumented code]

\subsection{Performance counters}
\label{subsection:performancecounters}

What they are, how they are used. Examples from modern x86 architectures maybe. Oprofile et. al.

pap182.pdf [Memory profiling using hardware counters.]

papi-ugc2001.pdf [PAPI cross-platform interface to hardware performance counters]

\subsubsection{Hardware-assisted profiling}
\label{subsubsection:hardwareassistedprofiling}

Not the same as performance counters. This one actually uses dedicated hardware to monitor different stuff rather than simple counters.

venkataramani\_hpca07.pdf [hardware for memory access monitoring and debugging]

LBA\_ISCA08.pdf [Flexible Hardware Acceleration for Instruction-Grain Program Monitoring]

LBA\_asid2006.pdf [Log-Based architectures for general-purpose monitoring of deployed code]

tacodec04.pdf [Architectural support for dynamic monitoring]

profiler.hpca.pdf [A programmable co-processor for profiling]

\subsection{Event-based profiling}
\label{subsection:eventbasedprofiling}

Events are triggered either through software or hardware exceptions. Give examples of how these might be used.

\section{Heap profiling}
\label{section:heapprofiling}

In section \ref{section:memlayout} we have presented the typical layout of a program after it has been loaded into memory. While in modern programs there exist a lot more sections than the ones described, the heap is usually the one where most of the allocations are done. Thus, we will not concern ourselves with the other sections because their size is pre-determined from compile time and they do not suffer modifications during run-time. In this subchapter we will present different methods of determining the size and the point in the program where allocations on the heap are performed.

\subsection{Allocation size profiling}
\label{subsubsection:allocationsizeprofiling}

The first problem we want to solve is the problem of determining the total size of all the data that exists on the heap. More specifically, we want to be able to answer one of our original questions: how much memory have the classes/modules allocated on the heap?

\subsubsection{Overloading memory allocation routines}
\label{subsubsection:overridingroutines}

A first solution to keeping track of all the allocations that a program has done is to overload the routines that do the allocations. By doing this, we can insert our own code in the routines, code which allows us to manipulate the allocation information in any way we want. The routines which have to be overloaded are the same ones presented in section \ref{section:heap}.

There are several problems with this approach, one of them related to the actual implementation of the mechanism. Overloading the routines means replacing them with our own while keeping the functionality intact. This has to be done in a way that is transparent to the running program and has very little overhead, preferably none. Different approaches exist:
\begin{itemize}
\item The \textit{new} and \textit{delete} operators can be overridden globally through language constructs provided by C++ itself. By looking at the way these two operators are implemented in the standard C++ library, one could provide an implementation that is identical but also provides additional profiling code.
\item For the \textit{malloc}, \textit{realloc} and \textit{free} routines, GCC provides hooks which allows their behaviour to be modified. These hooks are actually variables declared in malloc.h : \_\_malloc\_hook, \_\_realloc\_hook, \_\_free\_hook, \_\_memalign\_hook. All of these can point to independent routines which are called whenever the original allocation routines are called. These routines' signature contains a caller parameter which is the return address found on the stack when the allocation routines were called, thus allowing allocation point tracking\cite{GCCman}. The downside with using this method is that it is GCC specific, so if other compilers are used then either a similar mechanism has to exist for them or this approach does not work.
\item A separate library providing implementations for all the C-level allocation routines can be used. Since \textit{new} and \textit{delete} are also using these, they will also be taken into account, thus covering the whole range. This library can then be linked with the original program in such a way that the overloaded routines are used instead of the ones provided by the standard library. This is the approach that Valgrind uses, by exporting symbols which take precedence over the ones in glibc.so\cite{Seward02}. While it does have the benefit of being unintrusive it still is dependant on the build system, especially on the linker used.
\item Another solution is to provide wrappers for the allocation routines, which will be used instead of the original ones. The downside to this is that it is very intrusive since all of the original calls have to be replaced with calls to the wrappers. Tools that do this replacement automatically can be used.
\end{itemize}

Hunt and Brubacher\cite{Hunt99} classify techniques of intercepting function calls on Windows into four categories:
\begin{itemize}
\item \textit{Call replacement in application source code} - All of the above, except for the one involving providing a separate library fit into this category.
\item \textit{Call replacement in application binary code} - By using symbolic information call sites are identified and jump code to profiling routines can be inserted
\item \textit{DLL redirection} - Similar to using a separate library, the internals of this technique are Windows-specific
\item \textit{Breakpoint trapping} - By inserting a debugging breakpoint in the function we wish to intercept, we can have the debug exception handler reroute to a profiling routine. This involves a separate process (the Windows debugger) and it has the downside of suspending all application threads.
\end{itemize}
More than that, they compare these techniques with their interception implementation and show that the overhead varies from 250ns to 400ns with call replacement and DLL redirection, while breakpoint trapping has an overhead on the order of microseconds. If we add to this the fact that the profiling routine itself induces overhead, along with the fact that it proves to be not-trivial to implement and sometimes even intrusive, we can conclude that overloading the memory allocation routines in order to obtain live heap information is not a viable solution.

\subsubsection{On-demand memory tracking}
\label{subsubsection:ondemandtracking}

We now take a different approach to keeping track of the amount of allocated memory, one which does not involve interfering with the allocation routines. To do that, we note that most of the data living on the heap is structured in some way. Whether it is stored in just a simple array of integers or more complex data structures, it has references to it which can be accessed to determine its size. The advantage of such an approach is that we control when the size is determined and thus implicitly control when the overhead of this computation is imposed. The idea is to trigger the computation of the data structure's size on-demand, shifting the constant overhead of overloading memory allocation routines to a one-shot significantly larger overhead which could potentially be triggered during a period of low processor utilization.

The first possible way of keeping track of a data structure's size is \textit{counter-based}. This is as simple as keeping a variable which keeps track of the size that the data structure occupies, counter which is updated accordingly for each modification of said data structure. For example, an addNode function for a linked list would increment the variable with the size of the newly added node, while a removeNode function would decrement it in a similar manner. Naturally, more complex structures would require perhaps more counters and an even more careful accounting method, but the idea is the same: have a set of variables which accurately represent the size of the data structure at any point in time. The biggest advantage of this method is that it has very low overhead. The bulk of the accounting is spread between the methods which update the data structure and usually this involves only incrementing or decrementing the variables. When the information related to the data structure's size is required on-demand, all there is to do is to return the variables which contain this information, making this approach very lightweight in terms of overhead. The downsides are that it is intrusive, but, more importantly, it is very hard to maintain. Experience has shown \cite{Nethercote12} that people forget to update the profiling code when the data structure is updated, or partially update the profiling code since it is spread out in many methods that have an impact on the data structure. This leads to incorrect reporters that might not even be acknowledged as incorrect until after some serious debugging.

Since the main problem with the above method was that the profiling was spread in so many places that it was hard to keep track of all of them when they needed to be updated, perhaps there is a way to aggregate all of the profiling into one place. This is the idea with \textit{traversal-based} profiling. Have one method (or several, if multiple statistics are monitored) which traverses the data structure and reports its size. This does have significantly larger overhead than the above technique, especially if the data structure is large, but it is easier to maintain. Also, let's not forget that the idea is to trigger this traversal on-demand. There are several complicating factors with this approach, such as:
\begin{itemize}
\item Cycles in the data structure could lead to the same memory being counted twice
\item When using inheritance, the sub-classes must make sure not to take into account the memory of their parents again
\item Complex structures require complex traversals which are not trivial to implement and therefore might be difficult to maintain too
\end{itemize}

Note that by using these methods we have now lost the ability to detect memory leaks. If we would have kept track of every allocation then this extension would have been possible with some effort. However, this was never the purpose of this thesis so memory leak detection is out of scope. Allocations which are done and then never freed and do not have a reference to them will still continue to live on the heap and will occupy space but will not be detected by the profiler. This is considered a programmer error and specialized tools for their detection do exist.

In conclusion, both of the above methods are highly intrusive, requiring access to the source code. Counter-based profiling is the lightest of the two, but the hardest to maintain, while traversal-based has a higher overhead but better maintainability. Which one should be chosen is a matter of the project's size and priorities.

\subsection{Allocation point profiling}
\label{subsection:allocationpointprofiling}

The second problem we want to solve is to be able to answer two of our initial questions: who did the allocation and what lead to the allocation being done? The answer to both of these questions is found in the stack trace from the moment the allocation is done.

\subsubsection{Manual stack traversal}
\label{subsubsection:manualstacktraversal}

As we have seen in section \ref{section:stack}, the stack is where we can find information about the call chain that led to an allocation. Accessing the stack is, unfortunately, not a straight forward endeavour, mostly because each platform has subtle differences in the way the stack is implemented, which makes accessing it a bit difficult. Some compilers provide already implemented routines which hide away the details of the underlying architecture. One such example is GCC, which provides the \textit{backtrace} function. This function returns the call chain in a buffer of a given size. What it actually does behind the scenes is perform a stack walk.

A piece of software which does not want to be tied to a specific compiler should not use such compiler-provided functions but instead opt to implement its own. To give an idea of the complexity of a stack walker we now present the C implementation of such a program on an x86 Linux platform.

Keeping in mind the structure of a stack frame, described in section \ref{section:stack}, we need to determine two things:
\begin{itemize}
\item how to jump from stack frame to stack frame
\item how to obtain the return address from each frame, knowing that this return address is what determines the caller
\end{itemize}

Knowing only the beginning of the stack is of no use to us since we do not know how much local data has been pushed on the stack and therefore cannot determine precisely where the return address is. In this case, we can use the ebp register which is commonly used to point at the beginning of the current local data. However, we know that just above the local data lie the ebp value of the caller and the return address. Thus, to jump from stack frame to stack frame we have to follow the ebp values and to get the return address we just look at the value above the ebp on the stack.

Without going into the exact implementation details, a solution that does this is shown in \ref{stackwalker}:
\begin{lstlisting}[label=stackwalker,caption=Simple stack walker for x86]

struct frame {
        struct frame* old_fp;
        long ip;
};

struct frame *frame, *fp;

asm("movl %%ebp, %0" : "=r"(frame));

fp = frame;

for(; !(fp < frame) && !(fp < stack_bottom));
      fp = (struct frame *)((long)fp->old_fp))
   {
       // Do something with the return address from fp->ip
   }
\end{lstlisting}

The end result is that we can obtain a list of return addresses which can then be further used to obtain the actual names of the routines forming the call chain. Inserting the above routine in every allocation point would give us a stack trace which can be used to determine the exact call chain leading to the allocation.

There are, however, several downsides to this approach. First of all it is heavily platform dependant. The above code only runs on x86, using specific GCC directives. Not only that, but it relies on the fact that the code has been compiled with frame pointers activated. Some compiler-level optimizations remove the frame pointers to reduce stack frame size and obtain a small increase in speed. Different hardware platform may have a completely different stack frame format so the code would have to be rewritten for each compiler/platform combination, leading to something that would probably be very hard to maintain. The second problem is overhead. Attaching this code to every allocation point can lead to unnecessary overhead especially if we are not interested in the associated stack traces. A better method would be to activate the stack tracing on-demand, just for those allocations in which we are interested.

\subsubsection{Low overhead tracepoints}
\label{subsubsection:lowoverheadtracepoints}

The problem of low overhead tracepoints has been under discussion for a long time, especially in the context of debugging. The DTrace tool for Solaris allows probes to be inserted into a running program which have low overhead when they are disabled\cite{Cantrill04}. Such implementations have also been attempted on SPARC\cite{Kessler90} and the LTTng project had a series of tools dedicated to tracing Linux both in userspace and kernel\cite{Desnoyers09}. We will present the approach currently taken by the Linux kernel in this section.

A naive implementation of an on-demand triggerable tracepoint would just check the truth value of a flag and, based on that value, either call the tracing routine or not. It could be something as simple as the code in \ref{tracepoint0}. There are some problems which stem from this such as the need of a data structure which keeps a list of all the available tracepoints and implements some naming scheme allowing the user to enable/disable them independently. The question is if there is some way to avoid the condition check so that a disabled tracepoint would have even lower overhead.
\begin{lstlisting}[label=tracepoint0,caption=Naive tracepoint implementation]
...
if (tracepoint_enabled)
	trace();
...
\end{lstlisting}

The idea is to keep a list of all the statically defined tracepoints. In our case, since we want all allocation points to be traceable, there will be a tracepoint for each of them. This list is built by the compiler during compilation and placed in a special section in the executable which can be accessed during runtime. At the same time, tracepoints which are disabled are replaced with nop operations. To activate a tracepoint during runtime one has to lookup its address in the tracepoint table and replace the nop instruction from that address with a jump to the place in the code which calls the tracing function. The key to having this work is special compiler support for moving code which can be jumped to, but not accessed directly, out of line\cite{GCCasmgoto}. The listing from \ref{tracepoint1} shows the way this is done in the Linux kernel, along with a typical usage scenario.
\begin{lstlisting}[label=tracepoint1,caption=Linux kernel jump label implementation]
static __always_inline bool static_branch(struct jump_label_key *key)
{
        asm goto("0: nop\n\t"
                 ".pushsection __jump_table, \"aw\" \n\t"
                 ".balign 4 \n\t"
                 ".long 0b, %l[l_yes], %c0 \n\t"
                 ".popsection \n\t"
                 : : "i" (key) : : l_yes);
        return false;
l_yes:
        return true;
}

#define TRACE(name)
        static const char
        __tpname_##name[]
        __attribute__((section("__tracepoints_strings"))) =
                #name;

        static struct tracepoint
        __tracepoint_##name
        __attribute__((section("__tracepoints"))) =
                { 0, __tpname_##name, { 0 } };

        static struct tracepoint * const
        __tracepoint_ptr_##name
        __attribute__((section("__tracepoints_ptrs"))) =
                &__tracepoint_##name;

        if (static_branch(&__tracepoint_##name.key))
                trace(&__tracepoint_##name);
\end{lstlisting}

It works by inserting a nop at the label defined by \textit{0:}. In the section called \textit{\_\_jump\_table} we save the address of that label, the address of the label to jump to when the tracepoint is activated, along with a key identifying the tracepoint uniquely. Since the routine is typically used in a branch, and since it will always evaluate to false, a compiler will want to remove the code completely since it is unreachable. However, due to the jump to the \textit{l\_yes} label, it is not removed completely but moved somewhere out of line thus leaving only the nop instruction in place. We know where the code is moved because we have saved the address of the \textit{l\_yes} label, thus, in order to activate the tracepoint we have to replace the nop with a jump to that address.

This implementation shows that it is possible to achieve a tracepoint implementation whose only significant overhead is related to the code size of the nops and the out of line code. The downside is the same as the stack walker's: the implementation is platform specific. In this case it might even be worse since the optimization that the GCC compiler does by moving the code out of line and allowing labels inside an assembly block might not even be possible in other compilers. Permission to modify the program's code during runtime is also required and this might not be allowed in secure environments. The main issue is thus one of maintainability and of deciding if the cost of implementing and maintaining such a solution is indeed lower than the benefit of being able to trace call chains in key points of the program.

One final note to keep in mind is that the question we are asking is if it is possible to implement the above and tie it into a piece of software so that it can be used live and without damaging its performance. There are tools which already do this sort of tracing, such as DTrace mentioned above and even Valgrind so it is not an issue of implementation but rather performance and maintainability.

\subsubsection{Global stack object}
\label{subsubsection:globalstackobject}

The above two solutions suffer most on the maintainability side because of their platform dependencies. The question which naturally follows is if we can abstract away those parts into something which is independent of the platform we are running on. The answer to this would be to keep our own pseudo-stack (or stacks in case of multiple threads) which is globally accessible and can be queried regarding its state at any time. We say pseudo-stack because we would only be keeping the function names in it since that is what we are interested in. To have this working, each function must call one routine at its entry point pushing its name on the stack and another at its exit point for popping. A tool which inserts these calls automatically can be created.

The global stack object thus removes the need of having a stack walker. However, invoking the object for providing the call chain still requires the tracepoints and making these platform independent leads us to the naive implementation from listing \ref{tracepoint0}.

\section{Test program description}
\label{section:testprogram}
